{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewBertQA.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30rGof1zGt8Q",
        "outputId": "d0b4ff03-bcd5-4147-8429-df541a144255"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4XWHq2_mg8U",
        "outputId": "ca686c23-e0c9-42f3-e2c7-d4b8659106a2"
      },
      "source": [
        "pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 9.3MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.9.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtPm0UIREl4f",
        "outputId": "9d6b256f-288d-47fd-efa0-e44ecf468d74"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZiEPtQBFu8y",
        "outputId": "a426b0ab-9411-4a04-a639-c32f864b002e"
      },
      "source": [
        "pip install tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58fyTsBnlu8k"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULl5qAnemn_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3823b569-74f9-4d9d-81e3-ed9d6ea5c6b2"
      },
      "source": [
        "# squad, info = tfds.load('squad/v1.1', with_info=True, batch_size=-1)\n",
        "# print(squad.keys())\n",
        "train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
        "eval_path = keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\n",
        "with open(train_path) as f: raw_train_data = json.load(f)\n",
        "with open(eval_path) as f: raw_eval_data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "30294016/30288272 [==============================] - 0s 0us/step\n",
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "4857856/4854279 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTpXZ-jn5f2g"
      },
      "source": [
        "max_seq_length = 384\n",
        "input_word_ids_model = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
        "input_mask_model = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
        "input_type_ids_model = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLqzZjq7IfAh"
      },
      "source": [
        "pooled_output, sequence_output = bert_layer([input_word_ids_model, input_mask_model, input_type_ids_model])\n",
        "#vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
        "#vocab_file = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
        "#print(vocab_file)\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertWordPieceTokenizer(vocab='/content/bert-base-uncased-vocab.txt', lowercase=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvu_zZRz2CpA"
      },
      "source": [
        "def preprocess(data):\n",
        "  processed_data = []\n",
        "  for item in tqdm(data['data']):\n",
        "    for para in item['paragraphs']:\n",
        "      context = para['context']\n",
        "      for qas in para['qas']:\n",
        "        proc_data = {\n",
        "            'start_token_idx' :  -1,\n",
        "            'end_token_idx' : -1\n",
        "        }\n",
        "        answer = None\n",
        "        try:\n",
        "          answer = qas['answers'][0]['text']\n",
        "          answer_start = qas['answers'][0]['answer_start']\n",
        "        except:\n",
        "          pass\n",
        "        question = qas['question']\n",
        "        context = \" \".join(str(context).split())\n",
        "        question = \" \".join(str(question).split())\n",
        "        proc_data['context'] = tokenizer.encode(context)\n",
        "        proc_data['question'] = tokenizer.encode(question)\n",
        "        proc_data['raw_context'] = context\n",
        "        proc_data['raw_question'] = question\n",
        "        \n",
        "\n",
        "        if answer is not None:\n",
        "\n",
        "          answer_end = len(answer) + answer_start\n",
        "\n",
        "          #If the end of answer exceeds context.\n",
        "          if(answer_end >= len(context)):\n",
        "            continue\n",
        "\n",
        "          #Array of characters indicating where the answer is in the context.\n",
        "          answer_char_indices = [0] * len(context)\n",
        "          for idx in range(answer_start, answer_end):\n",
        "                answer_char_indices[idx] = 1\n",
        "\n",
        "          #Storing the encoded legal answer offsets (start and stop of encoded answer)\n",
        "          ans_token_idx = []\n",
        "          for idx, (start, end) in enumerate(proc_data['context'].offsets):\n",
        "                if sum(answer_char_indices[start:end]) > 0:\n",
        "                    ans_token_idx.append(idx)\n",
        "\n",
        "          #skip if there are no legal answers.\n",
        "          if len(ans_token_idx) == 0:\n",
        "            continue\n",
        "          \n",
        "          #Storing the start and end index of the tokenized(encoded) answer gotten from the context.\n",
        "          proc_data['start_token_idx'] = ans_token_idx[0]\n",
        "          proc_data['end_token_idx'] = ans_token_idx[-1]\n",
        "\n",
        "        #Setting up input_word_ids, input_type_ids, input_mask.\n",
        "        input_ids = proc_data['context'].ids + proc_data['question'].ids[1:]\n",
        "        token_type_ids = [0] * len(proc_data['context'].ids) + [1] * len(proc_data['question'].ids[1:])\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        if padding_length > 0:\n",
        "          input_ids = input_ids + ([0] * padding_length)\n",
        "          attention_mask = attention_mask + ([0] * padding_length)\n",
        "          token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "\n",
        "        #Skip if padding length is 0.\n",
        "        elif padding_length < 0:\n",
        "          continue\n",
        "        proc_data['input_word_ids'] = input_ids\n",
        "        proc_data['input_type_ids'] = token_type_ids\n",
        "        proc_data['input_mask'] = attention_mask\n",
        "        proc_data['context_token_to_char'] = proc_data['context'].offsets\n",
        "\n",
        "        processed_data.append(proc_data)\n",
        "  return processed_data\n",
        "\n",
        "def create_inputs_targets(processed_data):\n",
        "    dataset_dict = {\n",
        "        \"input_word_ids\": [],\n",
        "        \"input_type_ids\": [],\n",
        "        \"input_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in processed_data:\n",
        "          for key in dataset_dict:\n",
        "              dataset_dict[key].append(item[key])\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "    x = [dataset_dict[\"input_word_ids\"],\n",
        "         dataset_dict[\"input_mask\"],\n",
        "         dataset_dict[\"input_type_ids\"]]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ld8Nmad49UH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b94eda-58c4-42fa-b8c2-cf7410b8d635"
      },
      "source": [
        "#Creating the inputs targets.\n",
        "processed_data_train = preprocess(raw_train_data)\n",
        "processed_data_eval = preprocess(raw_eval_data)\n",
        "x_train, y_train = create_inputs_targets(processed_data_train)\n",
        "x_eval, y_eval = create_inputs_targets(processed_data_eval)\n",
        "\n",
        "#Adding the layer.\n",
        "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
        "start_logits = layers.Flatten()(start_logits)\n",
        "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
        "end_logits = layers.Flatten()(end_logits)\n",
        "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "model = keras.Model(inputs=[input_word_ids_model, input_mask_model, input_type_ids_model], outputs=[start_probs, end_probs])\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 442/442 [00:55<00:00,  8.03it/s]\n",
            "100%|██████████| 48/48 [00:06<00:00,  7.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_2 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "start_logit (Dense)             (None, 384, 1)       768         keras_layer_2[0][1]              \n",
            "__________________________________________________________________________________________________\n",
            "end_logit (Dense)               (None, 384, 1)       768         keras_layer_2[0][1]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 384)          0           start_logit[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 384)          0           flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 384)          0           flatten_7[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,777\n",
            "Trainable params: 109,483,776\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eLhk2ODxjOm"
      },
      "source": [
        "class ValidationCallback(keras.callbacks.Callback):\r\n",
        "\r\n",
        "    def normalize_text(self, text):\r\n",
        "        text = text.lower()\r\n",
        "        text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\r\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\r\n",
        "        text = re.sub(regex, \" \", text)\r\n",
        "        text = \" \".join(text.split())\r\n",
        "        return text\r\n",
        "\r\n",
        "    def __init__(self, x_eval, y_eval):\r\n",
        "        self.x_eval = x_eval\r\n",
        "        self.y_eval = y_eval\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\r\n",
        "        count = 0\r\n",
        "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\r\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\r\n",
        "            squad_eg = eval_examples_no_skip[idx]\r\n",
        "            offsets = squad_eg.context_token_to_char\r\n",
        "            start = np.argmax(start)\r\n",
        "            end = np.argmax(end)\r\n",
        "            if start >= len(offsets):\r\n",
        "                continue\r\n",
        "            pred_char_start = offsets[start][0]\r\n",
        "            if end < len(offsets):\r\n",
        "                pred_char_end = offsets[end][1]\r\n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\r\n",
        "            else:\r\n",
        "                pred_ans = squad_eg.context[pred_char_start:]\r\n",
        "            normalized_pred_ans = self.normalize_text(pred_ans)\r\n",
        "            normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]\r\n",
        "            if normalized_pred_ans in normalized_true_ans:\r\n",
        "                count += 1\r\n",
        "        acc = count / len(self.y_eval[0])\r\n",
        "        print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co7WMezXbLNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8983812-8e62-432e-88d3-6fef8c913f41"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=2, batch_size=8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "10767/10767 [==============================] - 8943s 829ms/step - loss: 3.5513 - activation_6_loss: 1.8330 - activation_7_loss: 1.7183\n",
            "Epoch 2/2\n",
            "10767/10767 [==============================] - 8936s 830ms/step - loss: 1.7106 - activation_6_loss: 0.9069 - activation_7_loss: 0.8037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Model.fit of <tensorflow.python.keras.engine.functional.Functional object at 0x7f723bf64470>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCit2Hc6Izuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c50cdba-6d29-48af-9e0d-30763a6f6b4d"
      },
      "source": [
        "model.save_weights(\"/content/drive/MyDrive/Bert2/weights.h5\")\n",
        "model.save(\"/content/drive/MyDrive/Bert2/mymodel\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 945). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 945). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Bert2/mymodel/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Bert2/mymodel/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7043I-Csxavz"
      },
      "source": [
        "# **Testing the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBVEF5MfxZ5j"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/gdrive/MyDrive/Bert2/mymodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I2L0WXKz00x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5fdb92-2897-4442-f677-799b4e496266"
      },
      "source": [
        "f = open(\"/content/context.txt\", \"r\")\n",
        "cont = f.read()\n",
        "data = {\"data\":\n",
        "    [\n",
        "        {\"title\": \"Project Apollo\",\n",
        "         \"paragraphs\": [\n",
        "             {\n",
        "                 \"context\": cont,\n",
        "                 \"qas\": [\n",
        "                     {\"question\": \"What makes gpu more efficient than cpu?\",\n",
        "                      \"id\": \"Q1\"\n",
        "                      }\n",
        "                 ]}]}]}\n",
        "# data = {\n",
        "#     'context' : [cont],\n",
        "#     'id' : '1',\n",
        "#     'question' : [\"What was Maria Curie the first female recipient of?\"]\n",
        "# }\n",
        "# x, _, offsets = preprocess(data)\n",
        "processedData = preprocess(data)\n",
        "x,y = create_inputs_targets(processedData)\n",
        "# print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 808.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTdUy1MZGLDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec655ffc-6382-4879-a903-906983c00d47"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzCBdFsw43PY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d12d0ab-1514-4514-866c-5302f6c839e3"
      },
      "source": [
        "pred_start, pred_end = model.predict(x)\n",
        "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "    test_sample = processedData[idx]\n",
        "    offset = test_sample['context_token_to_char']\n",
        "    start = np.argmax(start)\n",
        "    end = np.argmax(end)\n",
        "    pred_ans = None\n",
        "    if start >= len(offset):\n",
        "        continue\n",
        "    pred_char_start = offset[start][0]\n",
        "\n",
        "    if end < start:\n",
        "      #Get the sentence\n",
        "      lines = nltk.sent_tokenize(test_sample['raw_context'])\n",
        "      line_length = 0\n",
        "      print(test_sample['raw_context'][offset[start][0]:offset[start][1]])\n",
        "      for i, line in enumerate(lines):\n",
        "        line_length += len(line)\n",
        "        if offset[start][0] < line_length:\n",
        "          pred_ans = line\n",
        "          break\n",
        "    elif end < len(offset):\n",
        "      pred_ans = test_sample['raw_context'][pred_char_start:offset[end][1]]\n",
        "    else:\n",
        "        pred_ans = test_sample['raw_context'][pred_char_start:]\n",
        "    print(start, end, len(offset))\n",
        "    print(\"Q: \" + test_sample['raw_question'])\n",
        "    print(\"A: \" + pred_ans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "76 78 141\n",
            "Q: What makes gpu more efficient than cpu?\n",
            "A: highly parallel structure\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}